{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2b07fa-4707-4bfb-8037-6096ea649ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import json\n",
    "import collections\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    " \n",
    "sys.setrecursionlimit(5000) \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e8838e-1c7c-41a9-a1d3-b1142669fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_data(Data.Dataset):\n",
    "    def __init__(self,max_len=50,min_en_count=0,min_cn_count=0):\n",
    "        self.max_len = max_len\n",
    "        self.min_en_count = min_en_count\n",
    "        self.min_cn_count = min_cn_count\n",
    "        self.counter = None\n",
    "        self.cn_itos = ['<SOS>','<EOS>','<UNK>']\n",
    "        self.cn_stoi = {'<SOS>':0,'<EOS>':1,'<UNK>':2}\n",
    "        self.cn_data = []\n",
    "        self.en_itos = ['<SOS>','<EOS>','<UNK>']\n",
    "        self.en_stoi = {'<SOS>':0,'<EOS>':1,'<UNK>':2}\n",
    "        self.en_data = []\n",
    "        self.num_cn_vocab = 3\n",
    "        self.num_en_vocab = 3\n",
    "        self.len = 0\n",
    "        \n",
    "    def get_raw_data_cn_en(self,file,js=False, divide=1, choose=1):\n",
    "        all_cn = []\n",
    "        all_en = []\n",
    "        if js == False:\n",
    "            with open(file) as f:\n",
    "                lines = f.readlines()\n",
    "                data_len = len(lines)\n",
    "                k = int(data_len//divide)\n",
    "                start = max((choose-1)*k,0)\n",
    "                end = min(choose*k,data_len)\n",
    "                for line in lines[start:end]:\n",
    "                    cn,en = line.strip().split('\\t')\n",
    "                    en = en.lower()\n",
    "                    en = re.sub(r\"([,'.!?])\", r\" \\1\", en)\n",
    "                    en = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.,'!?]+\", r\" \", en)\n",
    "#                     cn = re.sub(r\"([.!?])\", r\" \\1\", cn)\n",
    "                    cn = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.,'‘’“”!?，。？！]+\", r\" \", cn)\n",
    "                    en=en.split()\n",
    "                    all_cn.append(cn)\n",
    "                    all_en.append(en)\n",
    "        if js == True:\n",
    "            with open(file) as f:\n",
    "                lines = f.readlines()\n",
    "                data_len = len(lines)\n",
    "                k = int(data_len//divide)\n",
    "                start = max((choose-1)*k,0)\n",
    "                end = min(choose*k,data_len)\n",
    "                for line in lines[start:end]:\n",
    "                    st = json.loads(line)\n",
    "                    cn = st['chinese']\n",
    "                    en = st['english'].lower()\n",
    "                    en = re.sub(r\"([,'.!?])\", r\" \\1\", en)\n",
    "                    en = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.,'!?]+\", r\" \", en)\n",
    "#                     cn = re.sub(r\"([.!?])\", r\" \\1\", cn)\n",
    "                    cn = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.,'‘’“”!?，。？！]+\", r\" \", cn)\n",
    "                    en=en.split()\n",
    "                    all_cn.append(cn)\n",
    "                    all_en.append(en)\n",
    "        return all_cn,all_en\n",
    "    \n",
    "    def get_cn_en_stoi_itos(self,raw_cn,raw_en):\n",
    "        cnCounter = collections.Counter([tk for line in raw_cn for tk in line])\n",
    "        enCounter = collections.Counter([tk for line in raw_en for tk in line])\n",
    "        cnCounter = dict(filter(lambda x: x[1]>=self.min_cn_count,cnCounter.most_common()))\n",
    "        enCounter = dict(filter(lambda x: x[1]>=self.min_en_count,enCounter.most_common()))\n",
    "        \n",
    "        for tk,_ in cnCounter.items():\n",
    "            self.cn_itos.append(tk)\n",
    "        for tk,_ in enCounter.items():\n",
    "            self.en_itos.append(tk)\n",
    "        \n",
    "        self.cn_stoi = {tk:idx for idx,tk in enumerate(self.cn_itos)}\n",
    "        self.en_stoi = {tk:idx for idx,tk in enumerate(self.en_itos)}\n",
    "        \n",
    "        self.num_cn_vocab = len(self.cn_stoi)\n",
    "        self.num_en_vocab = len(self.en_stoi)\n",
    "        \n",
    "    def get_from_vocab(self,cn_file,en_file):\n",
    "        self.cn_itos = []\n",
    "        self.en_itos = []\n",
    "        \n",
    "        with open(cn_file) as f:\n",
    "            for line in f.readlines():\n",
    "                self.cn_itos.append(line.replace('\\n',''))\n",
    "        with open(en_file) as f:\n",
    "            for line in f.readlines():\n",
    "                self.en_itos.append(line.replace('\\n',''))\n",
    "        \n",
    "        self.cn_stoi = {tk: idx for idx,tk in enumerate(self.cn_itos)}\n",
    "        self.en_stoi = {tk: idx for idx,tk in enumerate(self.en_itos)}\n",
    "        \n",
    "        self.num_cn_vocab = len(self.cn_stoi)\n",
    "        self.num_en_vocab = len(self.en_stoi)\n",
    "                \n",
    "        \n",
    "    def get_data(self,raw_cn,raw_en):\n",
    "        self.cn_data = []\n",
    "        self.en_data = []\n",
    "        k = [0]*len(raw_cn)\n",
    "        for idx,line in enumerate(raw_cn):\n",
    "            if len(line) > self.max_len:\n",
    "                k[idx] = 1\n",
    "                continue\n",
    "            temp = []\n",
    "            temp.append(0)\n",
    "            for tk in line:\n",
    "                if tk not in self.cn_itos:\n",
    "                    tk = '<UNK>'\n",
    "                temp.append(self.cn_stoi[tk])\n",
    "            temp.append(1)\n",
    "            self.cn_data.append(temp)\n",
    "            \n",
    "        for idx,line in enumerate(raw_en):\n",
    "            if k[idx] == 1:\n",
    "                continue\n",
    "            temp = []\n",
    "            temp.append(0)\n",
    "            for tk in line:\n",
    "                if tk not in self.en_itos:\n",
    "                    tk = '<UNK>'\n",
    "                temp.append(self.en_stoi[tk])\n",
    "            temp.append(1)\n",
    "            self.en_data.append(temp)\n",
    "        self.len = len(self.cn_data)\n",
    "        \n",
    "    def append_data(self,raw_cn,raw_en):\n",
    "        k = [0]*len(raw_cn)\n",
    "        for idx,line in enumerate(raw_cn):\n",
    "            if len(line) > self.max_len:\n",
    "                k[idx] = 1\n",
    "                continue\n",
    "            temp = []\n",
    "            temp.append(0)\n",
    "            for tk in line:\n",
    "                if tk not in self.cn_itos:\n",
    "                    tk = '<UNK>'\n",
    "                temp.append(self.cn_stoi[tk])\n",
    "            temp.append(1)\n",
    "            self.cn_data.append(temp)\n",
    "            \n",
    "        for idx,line in enumerate(raw_en):\n",
    "            if k[idx] == 1:\n",
    "                continue\n",
    "            temp = []\n",
    "            temp.append(0)\n",
    "            for tk in line:\n",
    "                if tk not in self.en_itos:\n",
    "                    tk = '<UNK>'\n",
    "                temp.append(self.en_stoi[tk])\n",
    "            temp.append(1)\n",
    "            self.en_data.append(temp)\n",
    "        self.len = len(self.cn_data)\n",
    "        \n",
    "    def do_all(self,file,js=False):\n",
    "        cn,en = self.get_raw_data_cn_en(file=file,js=js)\n",
    "        self.get_cn_en_stoi_itos(cn,en)\n",
    "        self.get_data(cn,en)\n",
    "        \n",
    "    \n",
    "    def get_data_pair(self,idx):\n",
    "        return self.cn_data[idx],self.en_data[idx]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.get_data_pair(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c2bc69-71e8-423b-b4f5-de75dbefc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_2(nn.Module):\n",
    "    def __init__(self,num_vocab,d_model=512,nhead=2,num_layers=2,dropout=0.1):\n",
    "        super(Encoder_2,self).__init__()\n",
    "        self.embedding = nn.Embedding(num_vocab,d_model)\n",
    "        self.MH = nn.TransformerEncoderLayer(d_model=d_model,nhead=nhead,dropout=dropout)\n",
    "        self.all_MH = nn.TransformerEncoder(self.MH,num_layers)\n",
    "        self.position_embedding = self.get_position_embedding(d_model,100).to(device)\n",
    "                                       \n",
    "    def forward(self,myinput,mask=None):\n",
    "        X = self.embedding(myinput)\n",
    "        X += self.position_embedding[:X.shape[1]]\n",
    "        X = X.permute(1,0,2)\n",
    "        #X shaep(L, Batch_size, d_model)\n",
    "        out = self.all_MH(X,src_key_padding_mask=mask)\n",
    "        return out\n",
    "    \n",
    "    def get_position_embedding(self,d_model,max_len):\n",
    "        table = torch.empty(max_len,d_model)\n",
    "        for position in range(max_len):\n",
    "            for i in range(d_model):\n",
    "                table[position,i] = position/10000**(i/d_model)\n",
    "        table[:,0::2] = torch.sin(table[:,0::2])\n",
    "        table[:,1::2] = torch.sin(table[:,1::2])\n",
    "        return table.float()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55018407-c882-4e79-ac92-a748597a5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_2(nn.Module):\n",
    "    def __init__(self,num_vocab,d_model=512,nhead=2,num_layers=2,dropout=0.1):\n",
    "        super(Decoder_2,self).__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.embedding = nn.Embedding(num_vocab, d_model)\n",
    "        self.MH = nn.TransformerDecoderLayer(d_model=d_model,nhead=nhead,dropout=dropout)\n",
    "        self.all_MH = nn.TransformerDecoder(self.MH,num_layers)\n",
    "        self.dense = nn.Linear(d_model,num_vocab)\n",
    "        self.position_embedding = self.get_position_embedding(d_model,100).to(device)\n",
    "        \n",
    "    def forward(self,target,memory,t_mask=None,m_mask=None,tgt_mask=None):\n",
    "        X = self.embedding(target)\n",
    "        X += self.position_embedding[:X.shape[1]]\n",
    "        X = X.permute(1,0,2)\n",
    "        Y = self.all_MH(X,memory,tgt_key_padding_mask=t_mask,memory_key_padding_mask=m_mask,tgt_mask=tgt_mask)\n",
    "        out = self.dense(Y.permute(1,0,2))\n",
    "        return out\n",
    "    \n",
    "    def get_position_embedding(self,d_model,max_len):\n",
    "        table = torch.empty(max_len,d_model)\n",
    "        for position in range(max_len):\n",
    "            for i in range(d_model):\n",
    "                table[position,i] = position/10000**(i/d_model)\n",
    "        table[:,0::2] = torch.sin(table[:,0::2])\n",
    "        table[:,1::2] = torch.sin(table[:,1::2])\n",
    "        return table.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32518e8e-20c3-4ac1-bc0b-c7ed17c06d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_mask(L):\n",
    "    return torch.from_numpy(np.triu(np.ones(L),k=1)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82096461-147a-4a2b-9d53-fdd9f5311bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6457\n",
      "25769\n",
      "6457\n",
      "25769\n",
      "0\n",
      "一\n",
      "5606\n"
     ]
    }
   ],
   "source": [
    "trainfile = 'translation2019zh/translation2019zh_train.json'\n",
    "trainfile2 = 'cn-eng.txt'\n",
    "\n",
    "\n",
    "trainset = My_data(min_en_count=8,min_cn_count=0)\n",
    "# 3333trainset.do_all(trainfile, js=True)\n",
    "# raw_cn, raw_en = trainset.get_raw_data_cn_en(trainfile,js=True,divide=50,choose=1)\n",
    "# raw_cn2, raw_en2 = trainset.get_raw_data_cn_en(trainfile2,js=False)\n",
    "# raw_cn3 = raw_cn+raw_cn2\n",
    "# raw_en3 = raw_en+raw_en2\n",
    "# trainset.get_cn_en_stoi_itos(raw_cn3,raw_en3)\n",
    "\n",
    "\n",
    "cn_file = 'cn_vocab.txt'\n",
    "en_file = 'en_vocab.txt'\n",
    "trainset.get_from_vocab(cn_file,en_file)\n",
    "\n",
    "print(len(trainset.cn_stoi))\n",
    "print(len(trainset.en_stoi))\n",
    "print(len(trainset.cn_itos))\n",
    "print(len(trainset.en_itos))\n",
    "print(len(trainset))\n",
    "print(trainset.cn_itos[7])\n",
    "print(trainset.en_stoi['bother'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a753192a-dcaf-462b-9596-b9c89949d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = Encoder_2(trainset.num_cn_vocab, d_model=512, nhead=4, num_layers=4).to(device)\n",
    "decoder = Decoder_2(trainset.num_en_vocab, d_model=512, nhead=4, num_layers=4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e909be8-66aa-40ab-b5b8-1d21e14006bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('TRANS07en.pth',map_location=device))\n",
    "decoder.load_state_dict(torch.load('TRANS07de.pth',map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1a5b80-0569-4156-921a-af0839f69711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Beam_search(decoder,memory,num_beam=10,max_len=200):\n",
    "    beam = []\n",
    "    \n",
    "    EOS = 1\n",
    "    PERIOD = 3\n",
    "    QU = 31\n",
    "    EX = 85\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        scores = torch.ones(num_beam,requires_grad=False)\n",
    "        for i in range(num_beam):\n",
    "            beam.append([0])\n",
    "\n",
    "        k = beam[1]\n",
    "        tgt_mask = get_decoder_mask(len(k)).to(device)\n",
    "        ans = decoder(torch.tensor(k).long().view(1,-1).to(device),memory,tgt_mask=tgt_mask).to('cpu')\n",
    "        ans = torch.nn.functional.softmax(ans.view(len(k),-1)[-1], dim=-1)\n",
    "        v1, d1 = torch.topk(ans, k=num_beam, dim=0)\n",
    "        for idx,d in enumerate(d1):\n",
    "            beam[idx].append(d)\n",
    "            scores[idx]=v1[idx].item()\n",
    "\n",
    "        for i in range(1,max_len):\n",
    "            values = []\n",
    "            indices = []\n",
    "            records = torch.ones(num_beam*num_beam)\n",
    "            for j in range(num_beam):\n",
    "                k = beam[j]\n",
    "                tgt_mask = get_decoder_mask(len(k)).to(device)\n",
    "                ans = decoder(torch.tensor(k).long().view(1,-1).to(device),memory,tgt_mask=tgt_mask).to('cpu')\n",
    "                ans = torch.nn.functional.softmax(ans.view(len(k),-1)[-1], dim=-1)\n",
    "                v1, d1 = torch.topk(ans, k=num_beam, dim=0)\n",
    "                values.append(v1)\n",
    "                indices.append(d1)\n",
    "                for idx,v in enumerate(v1):\n",
    "                    records[j*num_beam+idx] *= v\n",
    "                    records[j*num_beam+idx] *= scores[j]\n",
    "            v2, d2 = torch.topk(records,k=num_beam*num_beam,dim=-1)\n",
    "            new_beam_last = []\n",
    "            new_scores = []\n",
    "            old_beam_indices = []\n",
    "            for idx,d in enumerate(d2):\n",
    "                beam_id = int(d/num_beam)\n",
    "                old_beam_indices.append(beam_id)\n",
    "    #             temp.append(indices[beam_id][d-beam_id*num_beam].item())\n",
    "                new_beam_last.append(indices[beam_id][d-beam_id*num_beam].item())\n",
    "                new_scores.append(v2[idx])\n",
    "            counter = 0\n",
    "            candidate_beam = []\n",
    "            eos_set = []\n",
    "            for m in range(num_beam):\n",
    "                if beam[m][-1] == EOS :\n",
    "                    eos_set.append(m)\n",
    "                    list2 = beam[m][::]\n",
    "                    candidate_beam.append(list2)\n",
    "                    scores[counter] = scores[m]\n",
    "                    counter += 1\n",
    "                if beam[m][-1] == PERIOD: #or beam[m][-1] == QU or beam[m][-1] == EX:\n",
    "                    eos_set.append(m)\n",
    "                    beam[m].append(EOS)\n",
    "                    list2 = beam[m][::]\n",
    "                    candidate_beam.append(list2)\n",
    "                    scores[counter] = scores[m]\n",
    "                    counter += 1\n",
    "            if counter == num_beam:\n",
    "                break\n",
    "            for m,old_id in enumerate(old_beam_indices):\n",
    "                if counter >= num_beam:\n",
    "                    break\n",
    "                if old_id in set(eos_set):\n",
    "                    continue\n",
    "                list1 = beam[old_id][::]\n",
    "                list1.append(new_beam_last[m])\n",
    "                candidate_beam.append(list1)\n",
    "                scores[counter] = new_scores[m]\n",
    "                counter += 1\n",
    "            beam = candidate_beam\n",
    "#                 if beam[old][-1] != EOS :\n",
    "#                     beam[m].append(new_beam_last[m])\n",
    "#                     scores[m] = new_scores[m]\n",
    "#                     counter += 1\n",
    "                \n",
    "        \n",
    "    possibility,final_idx = torch.topk(scores, k=num_beam, dim=-1)\n",
    "    \n",
    "    return [beam[idx]for idx in final_idx],possibility\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ca711d-5907-4baf-8c45-64d8b0611938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polish(line):\n",
    "    line = line.strip().capitalize()\n",
    "    line = line.replace(' tom ',' Tom ')\n",
    "    line = line.replace(' lincoln ',' Lincoln ')\n",
    "    line = line.replace(' mary ',' Mary ')\n",
    "    line = line.replace(' anna ',' Anna ')\n",
    "    line = line.replace(' alice ',' Alice ')\n",
    "    line = line.replace(' emma ',' Emma ')\n",
    "    line = line.replace(' helen ',' Helen ')\n",
    "    line = line.replace(' eva ',' Eva ')\n",
    "    line = line.replace(' lisa ',' Lisa ')\n",
    "    line = line.replace(' james ',' James ')\n",
    "    line = line.replace(' john ',' John ')\n",
    "    line = line.replace(' robert ',' Robert ')\n",
    "    line = line.replace(' linda ',' Linda ')\n",
    "    line = line.replace(' william ',' William ')\n",
    "    line = line.replace(' david ',' David ')\n",
    "    line = line.replace(' richard ',' Richard ')\n",
    "    line = line.replace(' jason ',' Jason ')\n",
    "    line = line.replace(' jose ',' Jose ')\n",
    "    line = line.replace(' paul ',' Paul ')\n",
    "    line = line.replace(' maria ',' Maria ')\n",
    "    line = line.replace(' jerry ',' Jerry ')\n",
    "    line = line.replace(' jack ',' Jack ')\n",
    "    line = line.replace(' louis ',' Louis ')\n",
    "    line = line.replace(' joe ',' Joe ')\n",
    "    line = line.replace(' justin ',' Justin ')\n",
    "    line = line.replace(' mike ',' Mike ')\n",
    "    line = line.replace(' henry ',' Henry ')\n",
    "    line = line.replace(' benjamin ',' Benjamin ')\n",
    "    line = line.replace(' betty ',' Betty ')\n",
    "    line = line.replace(' smith ',' Smith ')\n",
    "    line = line.replace(' steve ',' Steve ')\n",
    "    line = line.replace(' susan ',' Susan ')\n",
    "    line = line.replace(' jane ',' Jane ')\n",
    "    line = line.replace(' sally ',' sally ')\n",
    "    line = line.replace(' julie ',' Julie ')\n",
    "    line = line.replace(' mrs ',' Mrs ')\n",
    "    line = line.replace(' mr ',' Mr ')\n",
    "    line = line.replace(' ms ',' Ms ')\n",
    "    line = line.replace(' miss ',' Miss ')\n",
    "    line = line.replace(' wi fi ',' Wi-Fi ')\n",
    "    line = line.replace(' u. s. ',' U.S. ')\n",
    "    line = line.replace(' america ',' America ')\n",
    "    line = line.replace(' canada ',' Canada ')\n",
    "    line = line.replace(' chinese ',' Chinese ')\n",
    "    line = line.replace(' china ',' China ')\n",
    "    line = line.replace(' japanese ',' Japanese ')\n",
    "    line = line.replace(' japan ',' Japan ')\n",
    "    line = line.replace(' france ',' France ')\n",
    "    line = line.replace(' french ',' French ')\n",
    "    line = line.replace(' u. k. ',' U.K. ')\n",
    "    line = line.replace(' english ',' English ')\n",
    "    line = line.replace(' british ',' British ')\n",
    "    line = line.replace(' england ',' England ')\n",
    "    line = line.replace(' britain ',' Britain ')\n",
    "    line = line.replace(' ireland ',' Ireland ')\n",
    "    line = line.replace(' scotland ',' Scotland ')\n",
    "    line = line.replace(' russia ',' Russia ')\n",
    "    line = line.replace(' egypt ',' Egypt ')\n",
    "    line = line.replace(' greece ',' Greece ')\n",
    "    line = line.replace(' germany ',' Germany ')\n",
    "    line = line.replace(' german ',' German ')\n",
    "    line = line.replace(' finland ',' Finland ')\n",
    "    line = line.replace(' sweden ',' Sweden ')\n",
    "    line = line.replace(' norway ',' Norway ')\n",
    "    line = line.replace(' iceland ',' Iceland ')\n",
    "    line = line.replace(' denmark ',' Denmark ')\n",
    "    line = line.replace(' poland ',' Poland ')\n",
    "    line = line.replace(' austria ',' Austria ')\n",
    "    line = line.replace(' switzerland ',' Switzerland ')\n",
    "    line = line.replace(' monaco ',' Monaco ')\n",
    "    line = line.replace(' italy ',' Italy ')\n",
    "    line = line.replace(' korea ',' Korea ')\n",
    "    line = line.replace(' singapore ',' Singapore ')\n",
    "    line = line.replace(' indonesia ',' Indonesia ')\n",
    "    line = line.replace(' iran ',' Iran ')\n",
    "    line = line.replace(' mexico ',' Mexico ')\n",
    "    line = line.replace(' greenland ',' Greenland ')\n",
    "    line = line.replace(' australia ',' Australia ')\n",
    "    line = line.replace(' australian ',' Australian ')\n",
    "    line = line.replace(' brazil ',' Brazil ')\n",
    "    line = line.replace(' asia ',' Asia ')\n",
    "    line = line.replace(' africa ',' Africa ')\n",
    "    line = line.replace(' antarctica ',' Antarctica ')\n",
    "    line = line.replace(' europe ',' Europe ')\n",
    "    line = line.replace(' oceania ',' Oceania ')\n",
    "    line = line.replace(\" m \",\"'m \")\n",
    "    line = line.replace(\" t \",\"'t \")\n",
    "    line = line.replace(\" s \",\"'s \")\n",
    "    line = line.replace(\" d \",\"'d \")\n",
    "    line = line.replace(\" ve \",\"'ve \")\n",
    "    line = line.replace(\" ll \",\"'ll \")\n",
    "    line = line.replace(\" re \",\"'re \")\n",
    "    line = line.replace(\" 'm \",\"'m \")\n",
    "    line = line.replace(\" 't \",\"'t \")\n",
    "    line = line.replace(\" 's \",\"'s \")\n",
    "    line = line.replace(\" 'd \",\"'d \")\n",
    "    line = line.replace(\" 've \",\"'ve \")\n",
    "    line = line.replace(\" 'll \",\"'ll \")\n",
    "    line = line.replace(\" 're \",\"'re \")\n",
    "    line = line.replace(' u ',' U ')\n",
    "    line = line.replace(' i ',' I ')\n",
    "    line = line.replace(' z ',' Z ')\n",
    "    line = line.replace(' c ',' C ')\n",
    "    line = line.replace(' b ',' B ')\n",
    "    line = line.replace(' f ',' F ')\n",
    "    line = line.replace(' g ',' G ')\n",
    "    line = line.replace(' h ',' H ')\n",
    "    line = line.replace(' j ',' J ')\n",
    "    line = line.replace(' k ',' K ')\n",
    "    line = line.replace(' l ',' L ')\n",
    "    line = line.replace(' o ',' O ')\n",
    "    line = line.replace(' p ',' P ')\n",
    "    line = line.replace(' q ',' Q ')\n",
    "    line = line.replace(' r ',' R ')\n",
    "    line = line.replace(' w ',' W ')\n",
    "    line = line.replace(' x ',' X ')\n",
    "    line = line.replace(' y ',' Y ')\n",
    "    line = line.replace(' ?','?')\n",
    "    line = line.replace(' .','.')\n",
    "    line = line.replace(' ,',',')\n",
    "    line = line.replace(' !','!')\n",
    "        \n",
    "    return line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65e23bd-6cf9-45c7-95cb-9dbd9cdfe1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(st,max_len = 50):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "#     t=[trainset.cn_stoi[tk] for tk in st if tk in trainset.cn_itos]\n",
    "    t=[]\n",
    "    for tk in st:\n",
    "        if tk not in trainset.cn_itos:\n",
    "            tk = '<UNK>'\n",
    "        t.append(trainset.cn_stoi[tk])\n",
    "        \n",
    "        \n",
    "    if t[0]!=0 :\n",
    "        t = [0]+t+[1]\n",
    "    \n",
    "    out = [0]\n",
    "    i = 1\n",
    "    t = torch.tensor(t).long().view(1,-1).to(device)\n",
    "    memory = encoder(t)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        tgt_mask = get_decoder_mask(i+1).to(device)\n",
    "        ans = decoder(torch.tensor(out).long().view(1,-1).to(device),memory,tgt_mask=tgt_mask).to('cpu')\n",
    "        q = torch.argmax(ans.view(-1,ans.shape[-1])[-1],dim=-1)\n",
    "        out.append(int(q))\n",
    "        if q == 1 :\n",
    "            break\n",
    "\n",
    "    out = [trainset.en_itos[idx] for idx in out]\n",
    "    return ' '.join(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c6a922-8dfc-4763-8879-c73c74febfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_evaluate(st,max_len = 50,long_st=False,ret=False,beam_width=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "#     t=[trainset.cn_stoi[tk] for tk in st if tk in trainset.cn_itos]\n",
    "    t = []\n",
    "    for tk in st:\n",
    "        if tk not in trainset.cn_itos:\n",
    "            tk = '<UNK>'\n",
    "        t.append(trainset.cn_stoi[tk])\n",
    "        \n",
    "    if t[0]!=0 :\n",
    "        t = [0]+t+[1]\n",
    "    \n",
    "    out = [0]\n",
    "    i = 1\n",
    "    t = torch.tensor(t).long().view(1,-1).to(device)\n",
    "    memory = encoder(t)\n",
    "    \n",
    "    outs,scores = Beam_search(decoder,memory,num_beam=beam_width,max_len=50)\n",
    "    \n",
    "    if ret == True:\n",
    "        out = outs[0]\n",
    "        out = out[1:-1]\n",
    "        out = [trainset.en_itos[idx] for idx in out]\n",
    "        return polish(' '.join(out))\n",
    "        \n",
    "    \n",
    "    if long_st == False:\n",
    "#         print('概率：\\n',scores)\n",
    "\n",
    "        outs = [[trainset.en_itos[idx] for idx in out]for out in outs]\n",
    "        for out in outs:\n",
    "            out = ' '.join(out[1:-1])\n",
    "            print(polish(out))\n",
    "            break\n",
    "\n",
    "        return outs\n",
    "    \n",
    "    else:\n",
    "        out = outs[0]\n",
    "        out = [trainset.en_itos[idx] for idx in out]\n",
    "\n",
    "        return out[1:-1],scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c4422e7-069a-424b-979e-50baf3ff449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_st_beam_search(st,split1=False):\n",
    "    \n",
    "    head = 0\n",
    "    end = 0\n",
    "    temp = []\n",
    "    if st[-1] != '。' and st[-1] != '！' and st[-1] != '？':\n",
    "        st = st+'。'\n",
    "    for tk in st:\n",
    "        end += 1\n",
    "        if tk == '！' or tk == '。' or tk =='？' or tk =='；':\n",
    "            temp.append(st[head:end])\n",
    "            head = end\n",
    "    \n",
    "    st = temp\n",
    "    temp = []\n",
    "    for min_st1 in st:\n",
    "        min_st2 = min_st1.replace('，','')\n",
    "#         if min_st2 != min_st1:\n",
    "#             out1, score1 = beam_search_evaluate(min_st1,long_st=True)\n",
    "#             out2, score2 = beam_search_evaluate(min_st2,long_st=True)\n",
    "#             if len(out1)>len(out2):\n",
    "#                 temp.append(out1)\n",
    "#             else:\n",
    "#                 temp.append(out2)\n",
    "#         else:\n",
    "        out1, score1 = beam_search_evaluate(min_st1,long_st=True)\n",
    "        temp.append(out1)\n",
    "    \n",
    "    if split1 == False:\n",
    "        out = ''\n",
    "        for idx,k in enumerate(temp):\n",
    "            k = ' '.join(k)\n",
    "            k = polish(k)\n",
    "            if idx == 0:\n",
    "                out = k\n",
    "            else :\n",
    "                out = out + ' ' + k\n",
    "        return out\n",
    "    \n",
    "    if split1 == True:\n",
    "        out = []\n",
    "        for idx,k in enumerate(temp):\n",
    "            k = ' '.join(k)\n",
    "            k = polish(k)\n",
    "            out.append(k)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9e3169-6321-4599-b373-91f585ed63a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is monday and not tuesday.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beam_search_evaluate('今天是周一而不是周二。')\n",
    "# beam_search_evaluate('树新的蜜蜂。')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e44e0526-2fd6-4beb-bcbf-c74078f3ca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So long as you break the big model in advance into smaller modules, do every small module well and the model is about the same.\n",
      "Of course, every small model will sometimes have problems needing to be <unk>, but at this point only need to modify this section and the rest is not affected.\n"
     ]
    }
   ],
   "source": [
    "k = long_st_beam_search('只要事先将大的模型分解成一些小的模块,把每一个小模块都做好了,大模型就差不多了。当然每一个小的模型有时也会有问题,需要重新做,但此时仅仅需要修改这一部分,其他的部分并不受影响,此过程大概需要一天半的时间',split1=True)\n",
    "for w in k:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dde8a21-759e-48d2-8620-e92370f745af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So long as you break the big model in advance into smaller modules, do every small module well and the model is about the same.\n",
      "Of course, every small model will sometimes have problems needing to be <unk>, but at this point only need to modify this section and the rest is not affected.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149acd4c-6efd-4ec7-8da8-3d009474c009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b75b3b13-aff0-403b-9061-5ce3e3bf887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab03 = []\n",
    "# with open('/Users/feisen/PycharmProjects/NLP03/vocab.txt') as f:\n",
    "#     for line in f.readlines():\n",
    "#         line = line.strip()\n",
    "#         if line in trainset.en_stoi:\n",
    "#             vocab03.append(trainset.en_stoi[line])\n",
    "#         else:\n",
    "#             vocab03.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44212704-3996-4964-b650-07716856877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordvec = torch.zeros(len(vocab03),512)\n",
    "# for idx,i in enumerate(vocab03):\n",
    "#     if idx >= 0:\n",
    "#         wordvec[idx] = decoder.embedding.weight.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd170567-bed2-43e7-9976-7b6338f690cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordvec = np.array(wordvec)\n",
    "# with open('wordvec.txt','w') as f:\n",
    "#     for line in wordvec:\n",
    "#         for w in line:\n",
    "#             f.write(str(w))\n",
    "#             f.write(' ')\n",
    "#         f.write('\\n')\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d564ad8-9036-4a3b-b19f-7de40628f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# com300 = []\n",
    "# with open('combined.csv') as f:\n",
    "#     for line in f.readlines()[1:]:\n",
    "#         line=line.strip().split(',')\n",
    "#         com300.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56e13af0-6179-457b-b059-387e92e87afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores1 = []\n",
    "# scores2 = []\n",
    "# for tk1, tk2, score in com300:\n",
    "#     if tk1.lower() in trainset.en_stoi and tk2.lower() in trainset.en_stoi:\n",
    "#         v1 = decoder.embedding.weight.data[trainset.en_stoi[tk1.lower()]]\n",
    "#         v2 = decoder.embedding.weight.data[trainset.en_stoi[tk2.lower()]]\n",
    "#         s = torch.sum(v1*v2)/torch.sum(torch.sqrt(v1**2*v2**2))\n",
    "#         scores1.append(s)\n",
    "#         scores2.append(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "193b1e56-b97e-42fa-a89b-eb0cbe03efc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1852576552300105\n"
     ]
    }
   ],
   "source": [
    "# array = np.array(scores1,dtype=float)\n",
    "# data2 = np.array(scores2,dtype=float)\n",
    "\n",
    "# xy = np.sum(array*data2)/len(array)\n",
    "# x = np.sum(array)/len(array)\n",
    "# y = np.sum(data2)/len(array)\n",
    "# x2 = np.sum(array*array)/len(array)\n",
    "# y2 = np.sum(data2*data2)/len(array)\n",
    "\n",
    "# cov = xy-x*y\n",
    "# varX = x2-x*x\n",
    "# varY = y2-y*y\n",
    "# Correlation_coefficient = cov/np.sqrt(varX*varY)\n",
    "# print(Correlation_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07f79094-8ed4-4651-ba1d-b3bf2950591a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 9861 is out of bounds for dimension 0 with size 6457",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-f9ffa3d4077f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9861\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 9861 is out of bounds for dimension 0 with size 6457"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de4215be-16e1-4ebb-8a3b-fa35712b1f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25769, 512])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder.embedding.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c84a9-011e-43e9-b949-6a65377b3a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
